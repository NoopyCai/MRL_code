import requests
from bs4 import BeautifulSoup
import csv
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from urllib.parse import urljoin, urlparse
import urllib3

# ç¦ç”¨SSLè­¦å‘Šï¼ˆé–‹ç™¼ç’°å¢ƒå¯èƒ½æœ‰SSLå•é¡Œï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

def check_url_status(url, base_url=None):
    try:
        if base_url and not url.startswith(('http://', 'https://')):
            url = urljoin(base_url, url)
            
        # å¢åŠ sessionå’Œæ›´å¯¬é¬†çš„è¨­å®š
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        response = session.get(url, timeout=20, verify=False, allow_redirects=True)
        
        # æ›´ç´°ç·»çš„ç‹€æ…‹åˆ¤æ–·
        if response.status_code == 404:
            return {
                'url': url,
                'status': '404_not_found',
                'h1_text': None,
                'error': f'404 Not Found',
                'status_code': response.status_code
            }
        elif response.status_code >= 500:
            return {
                'url': url,
                'status': 'server_error',
                'h1_text': None,
                'error': f'Server Error: {response.status_code}',
                'status_code': response.status_code
            }
        elif 200 <= response.status_code < 400:
            # 200-399 éƒ½è¦–ç‚ºå¯è¨ªå•
            soup = BeautifulSoup(response.text, 'html.parser')
            h1_tags = soup.find_all('h1')
            h1_texts = [h1.text.strip() for h1 in h1_tags]
            
            return {
                'url': url,
                'status': 'accessible',
                'h1_text': h1_texts[0] if h1_texts else 'No H1 found',
                'error': None,
                'status_code': response.status_code
            }
        else:
            # å…¶ä»–ç‹€æ…‹ç¢¼
            return {
                'url': url,
                'status': 'client_error',
                'h1_text': None,
                'error': f'Client Error: {response.status_code}',
                'status_code': response.status_code
            }
        
    except requests.exceptions.Timeout:
        return {
            'url': url,
            'status': 'timeout',
            'h1_text': None,
            'error': 'Request timeout',
            'status_code': None
        }
    except requests.exceptions.ConnectionError:
        return {
            'url': url,
            'status': 'connection_error',
            'h1_text': None,
            'error': 'Connection error',
            'status_code': None
        }
    except Exception as e:
        return {
            'url': url,
            'status': 'unknown_error',
            'h1_text': None,
            'error': str(e),
            'status_code': None
        }

def check_multiple_urls(urls, base_url=None, max_workers=10):  # é™ä½ä½µç™¼æ•¸
    results = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_url = {executor.submit(check_url_status, url, base_url): url 
                         for url in urls}
        
        for future in as_completed(future_to_url):
            result = future.result()
            results.append(result)
            print(f"å·²æª¢æŸ¥: {result['url']} - ç‹€æ…‹: {result['status']} - HTTP: {result.get('status_code', 'N/A')}")
            
            # æ·»åŠ å°å»¶é²é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
            time.sleep(0.1)
    
    return results

# ä¿å­˜çµæœåˆ°csv
def save_results_to_csv(results, filename='check_results.csv'):
    with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
        fieldnames = ['url', 'status', 'h1_text', 'error', 'status_code']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        writer.writeheader()
        for result in results:
            writer.writerow(result)
    
    print(f"\nçµæœå·²ä¿å­˜åˆ° {filename}")


if __name__ == "__main__":
    # æ­£ç¢ºè®€å–CSVæ–‡ä»¶çš„page_urlæ¬„ä½
    urls_to_check = []
    with open('catalog_product_entity.csv', 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            if 'page_url' in row and row['page_url']:
                urls_to_check.append(row['page_url'].strip())
    
    print(f"é–‹å§‹æª¢æŸ¥ {len(urls_to_check)} å€‹URL...")
    start_time = time.time()
    
    # æ·»åŠ åŸºç¤ç¶²å€
    base_url = 'http://dev-release-m2v243.mrliving.tw'  # ä¿®æ”¹ç‚ºä½ çš„ç¶²ç«™ç¶²å€
    results = check_multiple_urls(urls_to_check, base_url, max_workers=10)
    
    # çµ±è¨ˆçµæœ
    accessible_count = sum(1 for r in results if r['status'] == 'accessible')
    not_found_count = sum(1 for r in results if r['status'] == '404_not_found')
    server_error_count = sum(1 for r in results if r['status'] == 'server_error')
    other_error_count = sum(1 for r in results if r['status'] not in ['accessible', '404_not_found', 'server_error'])
    
    print(f"\næª¢æŸ¥å®Œæˆï¼è€—æ™‚: {time.time() - start_time:.2f} ç§’")
    print(f"ç¸½å…±æª¢æŸ¥: {len(results)} å€‹é é¢")
    print(f"âœ… å¯æ­£å¸¸è¨ªå•: {accessible_count} å€‹")
    print(f"âŒ 404éŒ¯èª¤: {not_found_count} å€‹")
    print(f"ğŸ”¥ æœå‹™å™¨éŒ¯èª¤: {server_error_count} å€‹")
    print(f"âš ï¸  å…¶ä»–éŒ¯èª¤: {other_error_count} å€‹")
    
    # ä¿å­˜çµæœ
    save_results_to_csv(results)
    
    # åˆ†é¡é¡¯ç¤ºéŒ¯èª¤
    print("\n=== 404 Not Found ===")
    for result in results:
        if result['status'] == '404_not_found':
            print(f"- {result['url']}")
    
    print("\n=== æœå‹™å™¨éŒ¯èª¤ ===")
    for result in results:
        if result['status'] == 'server_error':
            print(f"- {result['url']} (HTTP {result.get('status_code', 'N/A')})")
    
    print("\n=== å…¶ä»–éŒ¯èª¤ ===")
    for result in results:
        if result['status'] not in ['accessible', '404_not_found', 'server_error']:
            print(f"- {result['url']} ({result['status']}: {result['error']})")